My Dataflow Pipeline Documentation
Pipeline Overview
Purpose: This pipeline is designed to ingest historical NVIDIA (NVDA) stock data from a CSV file stored in Google Cloud Storage, perform necessary data type conversions and formatting, and load the processed data into a BigQuery table for analytical querying. It ensures data integrity by handling potential parsing errors and correctly mapping source data to the target schema.

Source: The pipeline reads data from a CSV file named NVidia_stock_history.csv located within the pipeline_input folder of the gs://mgmt599-justinrizzzo-data-lake-lab2 Cloud Storage bucket.

Transformations: The following transformations are applied to each record:

Decoding: Each raw line read from the CSV file is initially decoded from latin1 encoding to a standard Python string, addressing potential character encoding issues.

CSV Parsing: The decoded string is then parsed as a CSV row using csv.reader from StringIO, converting it into a list of string values.

Header Skipping: The first line (header) of the input CSV file is skipped before processing.

Data Type Conversion & Formatting: Individual fields are converted to their appropriate data types:

Open, High, Low, Close, Dividends, and Stock_Splits are converted to float.

Volume is converted to integer.

The 'Date' field is parsed from its string format (%Y-%m-%d %H:%M:%S%z), which includes timezone information (e.g., -05:00), into a Python datetime object. This datetime object is then converted into an ISO 8601 formatted string (YYYY-MM-DDTHH:MM:SS.ffffff[+|-]HH:MM), which is the standard representation for BigQuery's TIMESTAMP type.

Error Handling: A robust try-except block is implemented within the parse_csv_and_transform function. This catches and logs any errors encountered during the parsing and transformation of individual rows, ensuring that problematic records do not halt the entire pipeline. Rows that cause errors are returned as empty dictionaries and subsequently filtered out before writing to BigQuery.

Destination: The processed and transformed data is written to the nvidia table within the nvidia dataset in the mgmt599-justinrizzo-lab2 BigQuery project. The pipeline is configured to overwrite the table on each run (WRITE_TRUNCATE) and create the table if it doesn't already exist (CREATE_IF_NEEDED).

Pipeline Configuration
Job name: The job name is dynamically generated by Dataflow based on the script name and timestamp (e.g., beamapp-justinrizzo90-YYYYMMDD-HHMMSS-XXXX).

Region: us-central1

Machine type: Dataflow uses default machine types (typically n1-standard-1 or similar) as no specific machine type was explicitly configured in the PipelineOptions.

Max workers: Dataflow's auto-scaling feature is enabled by default, adjusting the number of workers based on workload. No explicit maximum number of workers was set in the PipelineOptions.

Data Flow
Read from: gs://mgmt599-justinrizzzo-data-lake-lab2/pipeline_input/NVidia_stock_history.csv

Transform:

Read raw file content as text lines (beam.io.ReadFromText).

Explicitly decode each line from latin1 to string.

Parse each line as a CSV row using csv.reader from StringIO.

Skip the header row.

Convert Date string (e.g., 1999-01-22 00:00:00-05:00) to a Python datetime object, correctly parsing the timezone offset (%Y-%m-%d %H:%M:%S%z).

Convert the datetime object to an ISO 8601 formatted string.

Convert Open, High, Low, Close, Dividends, Stock_Splits to float.

Convert Volume to integer.

Filter out any records that failed parsing (e.g., empty dictionaries).

Write to: mgmt599-justinrizzo-lab2.nvidia.nvidia

Lessons Learned
What was challenging?

Debugging Data Type Mismatches: Repeated ValueError exceptions (e.g., invalid literal for int(), time data does not match format) highlighted the critical importance of precisely matching CSV column data types and formats (especially dates) to their Python conversion functions and the target BigQuery schema. This required iterative inspection of the raw CSV and refinement of the TransformData logic. The specific format of the Date column including timezone (%z) was a key detail that needed careful attention.

Character Encoding Issues: The UnicodeDecodeError was a significant hurdle, demonstrating that UTF-8 is not always the default or correct encoding, and explicit handling (e.g., latin1 decoding) is necessary for certain source files.

Apache Beam I/O Nuances: Understanding the specific methods and parameters for beam.io.ReadFromText and how they interact with encoding and header skipping proved challenging due to variations in Beam versions and their expected arguments. The use of StringIO and csv.reader within the Map transformation was a robust solution to ensure proper CSV parsing of each line.

Pipeline Development Cycle: The iterative process of writing code, deploying to Dataflow, waiting for job execution/failure, analyzing logs, and repeating, reinforced the need for efficient debugging strategies and detailed error message interpretation.

What would you do differently?

Pre-analysis of Source Data: Before writing the pipeline, perform a more exhaustive pre-analysis of the source CSV file, including:

Confirming the exact header row to ensure accurate skip_header_lines logic.

Thoroughly inspecting data types and formats for every column, especially dates (including timezone components) and numbers, to prevent ValueErrors.

Explicitly checking the file's encoding (e.g., using file -i filename in Linux or a text editor's encoding detection) to set the correct decoding early.

Modular Transformation Functions: While the current parse_csv_and_transform is effective, for more complex pipelines, breaking down transformations into smaller, testable DoFn classes or functions would improve maintainability and debugging.

Local Testing with Sample Data: Implement more robust local testing with small, representative sample data files to catch data parsing and transformation errors quickly, before deploying to Dataflow, which has a longer feedback loop.

Explicit Schema Definition: For BigQuery, consider defining the schema as a list of dictionaries (e.g., [{"name": "Date", "type": "TIMESTAMP"}, ...]) rather than a single string, which can sometimes be more readable and less error-prone for complex schemas, especially when dealing with nested structures.
